# core/classifier.py
import logging
import numpy as np
import pandas as pd
import joblib
import os
import re
from typing import List, Dict, Tuple, Any
from tqdm import tqdm

from .feature_extractor import HybridFeatureExtractor
from .model_trainer import ModelTrainer
from .data_processor import DataProcessor
from .config import Config


class NewsClassifier:
    """
    Hlavn√° trieda pre klasifik√°ciu titulkov do kateg√≥ri√≠ dezinform√°ci√≠
    Pou≈æ√≠va Slovak BERT + TF-IDF features a Random Forest
    """

    def __init__(self, use_bert: bool = True, use_tfidf: bool = True):
        """
        Inicializ√°cia klasifik√°tora

        Args:
            use_bert: Pou≈æi≈• BERT features
            use_tfidf: Pou≈æi≈• TF-IDF features
        """
        self.config = Config()
        self.logger = logging.getLogger(__name__)

        # Nastavenie feature extractoru
        self.config.USE_BERT = use_bert
        self.config.USE_TFIDF = use_tfidf

        # Inicializ√°cia komponentov
        self.feature_extractor = HybridFeatureExtractor()
        self.model_trainer = ModelTrainer()
        self.data_processor = DataProcessor()

        self.is_trained = False
        self.is_loaded = False

        self.logger.info(f"Classifier inicializovan√Ω - BERT: {use_bert}, TF-IDF: {use_tfidf}")
        self.logger.info(f"Pou≈æ√≠van√© zariadenie: {self.config.DEVICE}")

    def train(self, enable_augmentation: bool = True) -> Dict[str, Any]:
        """
        Kompletn√Ω pipeline tr√©novania

        Args:
            enable_augmentation: Povoli≈• roz≈°√≠renie d√°t

        Returns:
            Dictionary s v√Ωsledkami tr√©novania
        """
        self.logger.info("=== ZAƒå√çNAM TR√âNOVANIE KLASIFIK√ÅTORA ===")

        try:
            # 1. Naƒç√≠tanie a preprocessing d√°t
            df = self.data_processor.load_data()
            self.logger.info(f"Naƒç√≠tan√Ωch {len(df)} tr√©novac√≠ch pr√≠kladov")

            # 2. Data augmentation (voliteƒæn√©)
            if enable_augmentation:
                df = self.data_processor.augment_data(df)
                self.logger.info(f"Po augment√°cii: {len(df)} pr√≠kladov")

            # 3. Preprocessing
            X, y, original_labels = self.data_processor.preprocess_data(df)

            # 4. Rozdelenie d√°t
            X_train, X_test, y_train, y_test = self.data_processor.split_data(X, y)

            # 5. Feature extraction
            self.logger.info("Zaƒç√≠nam feature extraction...")
            self.feature_extractor.fit(X_train)
            X_train_features = self.feature_extractor.transform(X_train)
            X_test_features = self.feature_extractor.transform(X_test)

            self.logger.info(f"Tr√©novacie features: {X_train_features.shape}")
            self.logger.info(f"Testovacie features: {X_test_features.shape}")

            # 6. Tr√©novanie modelu
            trained_model = self.model_trainer.train_model(
                X_train_features, y_train, X_test_features, y_test
            )

            # 7. Ulo≈æenie modelov
            self._save_models()

            # 8. Evalu√°cia
            results = self._evaluate_training(X_test_features, y_test, original_labels)

            self.is_trained = True
            self.is_loaded = True

            self.logger.info("=== TR√âNOVANIE √öSPE≈†NE DOKONƒåEN√â ===")
            return results

        except Exception as e:
            self.logger.error(f"Chyba pri tr√©novan√≠: {e}")
            raise

    def _save_models(self):
        """Ulo≈æ√≠ v≈°etky natr√©novan√© modely a komponenty"""
        self.logger.info("Uklad√°m modely...")

        os.makedirs(self.config.MODELS_DIR, exist_ok=True)

        # Ulo≈æenie feature extractoru
        self.feature_extractor.save()

        # Ulo≈æenie klasifik√°tora
        self.model_trainer.save_model()

        # Ulo≈æenie label encoderu
        self.data_processor.save_label_encoder()

        # Ulo≈æenie konfigur√°cie
        config_path = os.path.join(self.config.MODELS_DIR, 'training_config.joblib')
        joblib.dump({
            'use_bert': self.config.USE_BERT,
            'use_tfidf': self.config.USE_TFIDF,
            'categories': self.config.CATEGORIES,
            'feature_dimensions': {
                'bert_original': self.config.BERT_EMBEDDING_DIM,
                'bert_reduced': self.config.REDUCED_BERT_DIM,
                'tfidf': self.config.TFIDF_MAX_FEATURES
            }
        }, config_path)

        self.logger.info(f"V≈°etky modely ulo≈æen√© do: {self.config.MODELS_DIR}")

    def _evaluate_training(self, X_test, y_test, original_labels) -> Dict[str, Any]:
        """Komplexn√° evalu√°cia tr√©novania"""
        from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
        import seaborn as sns
        import matplotlib.pyplot as plt

        # Predikcie
        y_pred = self.model_trainer.model.predict(X_test)
        y_pred_proba = self.model_trainer.model.predict_proba(X_test)

        # Metriky
        accuracy = accuracy_score(y_test, y_pred)
        report = classification_report(y_test, y_pred, output_dict=True)

        # Confusion matrix
        cm = confusion_matrix(y_test, y_pred)

        # V√Ωpoƒçet confidence scores
        confidence_scores = np.max(y_pred_proba, axis=1)
        avg_confidence = np.mean(confidence_scores)

        results = {
            'accuracy': accuracy,
            'classification_report': report,
            'confusion_matrix': cm,
            'average_confidence': avg_confidence,
            'test_set_size': len(X_test),
            'feature_dimensions': X_test.shape[1],
            'categories': self.data_processor.label_encoder.classes_.tolist()
        }

        # Log v√Ωsledkov
        self.logger.info(f"Koneƒçn√° presnos≈•: {accuracy:.4f}")
        self.logger.info(f"Priemern√° istota: {avg_confidence:.4f}")
        self.logger.info(f"Poƒçet features: {X_test.shape[1]}")

        return results

    def load_models(self) -> bool:
        """
        Naƒç√≠tanie natr√©novan√Ωch modelov

        Returns:
            True ak sa naƒç√≠tanie podarilo, inak False
        """
        try:
            self.logger.info("Naƒç√≠tavam natr√©novan√© modely...")

            # Kontrola existencie modelov
            required_files = [
                'trained_model.joblib',
                'label_encoder.joblib',
                'training_config.joblib'
            ]

            for file in required_files:
                if not os.path.exists(os.path.join(self.config.MODELS_DIR, file)):
                    self.logger.error(f"Ch√Ωbaj√∫ci s√∫bor: {file}")
                    return False

            # Naƒç√≠tanie komponentov
            self.feature_extractor.load()
            self.model_trainer.load_model()
            self.data_processor.load_label_encoder()

            # Naƒç√≠tanie konfigur√°cie
            config_path = os.path.join(self.config.MODELS_DIR, 'training_config.joblib')
            training_config = joblib.load(config_path)

            self.is_loaded = True
            self.logger.info("‚úÖ V≈°etky modely √∫spe≈°ne naƒç√≠tan√©")
            self.logger.info(f"üìä Kateg√≥rie: {', '.join(training_config['categories'])}")

            return True

        except Exception as e:
            self.logger.error(f"‚ùå Chyba pri naƒç√≠tavan√≠ modelov: {e}")
            return False

    def predict(self, text: str, return_confidence: bool = True) -> Tuple[str, Dict[str, float]]:
        """
        Predikcia kateg√≥rie pre jeden text

        Args:
            text: Text titulku na klasifik√°ciu
            return_confidence: Vr√°ti≈• pravdepodobnosti pre v≈°etky kateg√≥rie

        Returns:
            Tuple: (predpovedan√°_kateg√≥ria, pravdepodobnosti)
        """
        if not self.is_loaded and not self.load_models():
            raise ValueError("Modely nie s√∫ naƒç√≠tan√© a nepodarilo sa ich naƒç√≠ta≈•!")

        # ƒåistenie textu
        cleaned_text = self._clean_text(text)

        if not cleaned_text:
            raise ValueError("Text je pr√°zdny po vyƒçisten√≠!")

        # Feature extraction
        try:
            features = self.feature_extractor.transform([cleaned_text])
        except Exception as e:
            self.logger.error(f"Chyba pri feature extraction: {e}")
            # Fallback na z√°kladn√∫ klasifik√°ciu podƒæa kƒæ√∫ƒçov√Ωch slov
            return self._fallback_prediction(text)

        # Predikcia
        prediction = self.model_trainer.model.predict(features)[0]
        probabilities = self.model_trainer.model.predict_proba(features)[0]

        # Decode label
        predicted_label = self.data_processor.label_encoder.inverse_transform([prediction])[0]

        # Pravdepodobnosti pre v≈°etky kateg√≥rie
        prob_dict = {
            category: float(prob) for category, prob in zip(
                self.data_processor.label_encoder.classes_,
                probabilities
            )
        }

        # Logovanie vysokej istoty
        max_prob = max(probabilities)
        if max_prob > 0.8:
            self.logger.debug(f"Vysok√° istota ({max_prob:.3f}) pre: '{text}' -> {predicted_label}")

        return predicted_label, prob_dict

    def predict_batch(self, texts: List[str], show_progress: bool = True) -> List[Dict[str, Any]]:
        """
        Hromadn√° predikcia pre viacero textov

        Args:
            texts: Zoznam textov na klasifik√°ciu
            show_progress: Zobrazi≈• progress bar

        Returns:
            Zoznam v√Ωsledkov pre ka≈æd√Ω text
        """
        if not self.is_loaded and not self.load_models():
            raise ValueError("Modely nie s√∫ naƒç√≠tan√©!")

        if not texts:
            return []

        # ƒåistenie textov
        cleaned_texts = [self._clean_text(text) for text in texts]
        valid_texts = [text for text in cleaned_texts if text]

        if not valid_texts:
            raise ValueError("≈Ωiadny validn√Ω text po ƒçisten√≠!")

        # Feature extraction
        if show_progress:
            self.logger.info(f"Spracov√°vam {len(valid_texts)} textov...")

        features = self.feature_extractor.transform(valid_texts)

        # Predikcia
        predictions = self.model_trainer.model.predict(features)
        probabilities = self.model_trainer.model.predict_proba(features)

        # Decode labels
        predicted_labels = self.data_processor.label_encoder.inverse_transform(predictions)

        # Zostavenie v√Ωsledkov
        results = []
        iterator = zip(texts, predicted_labels, probabilities)

        if show_progress:
            iterator = tqdm(iterator, total=len(texts), desc="Klasifik√°cia")

        for original_text, label, probs in iterator:
            prob_dict = {
                category: float(prob) for category, prob in zip(
                    self.data_processor.label_encoder.classes_,
                    probs
                )
            }

            results.append({
                'original_text': original_text,
                'cleaned_text': self._clean_text(original_text),
                'predicted_category': label,
                'probabilities': prob_dict,
                'confidence': float(max(probs)),
                'is_high_confidence': max(probs) > 0.7
            })

        return results

    def _clean_text(self, text: str) -> str:
        """
        Z√°kladn√© ƒçistenie textu

        Args:
            text: Vstupn√Ω text

        Returns:
            Vyƒçisten√Ω text
        """
        if not isinstance(text, str):
            return ""

        # Odstr√°nenie prebytoƒçn√Ωch bielych znakov
        text = re.sub(r'\s+', ' ', text.strip())

        # Odstr√°nenie ≈°peci√°lnych znakov (ponech√° z√°kladn√© diakritiku)
        text = re.sub(r'[^\w\s√°√§ƒçƒè√©√≠ƒæƒ∫≈à√≥√¥≈ô≈ï≈°≈•√∫√Ω≈æ√Å√Ñƒåƒé√â√çƒΩƒπ≈á√ì√î≈ò≈î≈†≈§√ö√ù≈Ω\-\'",.!?;]', '', text)

        return text.lower()

    def _fallback_prediction(self, text: str) -> Tuple[str, Dict[str, float]]:
        """
        Z√°lo≈æn√° klasifik√°cia podƒæa kƒæ√∫ƒçov√Ωch slov (ak feature extraction zlyh√°)
        """
        self.logger.warning(f"Pou≈æ√≠vam fallback klasifik√°ciu pre: {text}")

        text_lower = text.lower()

        # Kƒæ√∫ƒçov√© slov√° pre ka≈æd√∫ kateg√≥riu
        keywords = {
            'clickbait': ['≈°okuj√∫ce', 'neuver√≠te', 'kliknite', 'zist√≠te', 'tajomstvo', 'odhalenie'],
            'false_news': ['z√°kaz', 'ru≈°√≠', 'zatvoren√©', 'zakazuje', 'zak√°zal', 'povinn√©'],
            'conspiracy': ['tajn√©', 'elity', 'ovl√°daj√∫', 'alien', 'mimozem≈°≈•an', 'bunkor'],
            'propaganda': ['jedin√°', 'z√°chrana', 'na≈°a strana', 'l√≠dra', '√∫spechy'],
            'satire': ['zru≈°en√Ω', 'zak√°zal d√°≈æƒè', 'povinn√© nosenie', 'bryndzov√© halu≈°ky'],
            'misleading': ['v≈°etk√Ωch', 'z√°zraƒçne', '√∫plne', 'absol√∫tne', '100%'],
            'biased': ['neschopn√Ω', 'zlyhal', 'podvodn√≠k', 'hl√∫py', 'kritizuje'],
            'legitimate': ['schv√°lila', 'ozn√°mil', 'vydalo', 'objavili', 'otvor√≠']
        }

        # Poƒçet zh√¥d pre ka≈æd√∫ kateg√≥riu
        scores = {category: 0 for category in self.config.CATEGORIES}

        for category, words in keywords.items():
            for word in words:
                if word in text_lower:
                    scores[category] += 1

        # N√°jdenie kateg√≥rie s najvy≈°≈°√≠m sk√≥re
        max_score = max(scores.values())
        if max_score > 0:
            predicted_category = max(scores, key=scores.get)
        else:
            predicted_category = 'legitimate'  # Default

        # Simul√°cia pravdepodobnost√≠
        total_score = sum(scores.values()) or 1
        prob_dict = {
            category: score / total_score for category, score in scores.items()
        }

        return predicted_category, prob_dict

    def get_model_info(self) -> Dict[str, Any]:
        """
        Z√≠skanie inform√°ci√≠ o natr√©novanom modeli

        Returns:
            Dictionary s inform√°ciami o modeli
        """
        if not self.is_loaded:
            return {"status": "Modely nie s√∫ naƒç√≠tan√©"}

        info = {
            "status": "Natr√©novan√Ω a naƒç√≠tan√Ω",
            "feature_extractor": {
                "uses_bert": self.config.USE_BERT,
                "uses_tfidf": self.config.USE_TFIDF,
                "bert_model": self.config.BERT_MODEL_NAME
            },
            "classifier": {
                "type": "RandomForest",
                "n_estimators": self.config.N_ESTIMATORS
            },
            "categories": self.config.CATEGORIES,
            "device": self.config.DEVICE
        }

        if hasattr(self.model_trainer, 'model') and self.model_trainer.model is not None:
            info["classifier"]["n_features"] = (
                self.model_trainer.model.n_features_in_
                if hasattr(self.model_trainer.model, 'n_features_in_')
                else "Unknown"
            )

        return info

    def evaluate_custom_data(self, texts: List[str], true_labels: List[str]) -> Dict[str, Any]:
        """
        Evalu√°cia modelu na vlastn√Ωch d√°tach

        Args:
            texts: Zoznam textov
            true_labels: Skutoƒçn√© kateg√≥rie

        Returns:
            V√Ωsledky evalu√°cie
        """
        from sklearn.metrics import classification_report, accuracy_score

        if not self.is_loaded:
            raise ValueError("Modely musia by≈• naƒç√≠tan√©!")

        if len(texts) != len(true_labels):
            raise ValueError("Texty a labels musia ma≈• rovnak√∫ dƒ∫≈æku!")

        # Predikcia
        results = self.predict_batch(texts, show_progress=True)
        predicted_labels = [result['predicted_category'] for result in results]

        # Metriky
        accuracy = accuracy_score(true_labels, predicted_labels)
        report = classification_report(true_labels, predicted_labels, output_dict=True)

        # Confusion matrix
        from sklearn.metrics import confusion_matrix
        cm = confusion_matrix(true_labels, predicted_labels,
                              labels=self.data_processor.label_encoder.classes_)

        return {
            'accuracy': accuracy,
            'classification_report': report,
            'confusion_matrix': cm,
            'sample_size': len(texts),
            'predictions': results
        }


# Jednoduch√° factory funkcia pre r√Ωchle vytvorenie klasifik√°tora
def create_classifier(use_bert: bool = True, use_tfidf: bool = True) -> NewsClassifier:
    """
    R√Ωchle vytvorenie klasifik√°tora

    Args:
        use_bert: Pou≈æi≈• BERT features
        use_tfidf: Pou≈æi≈• TF-IDF features

    Returns:
        Inicializovan√Ω NewsClassifier
    """
    return NewsClassifier(use_bert=use_bert, use_tfidf=use_tfidf)


# Testovacia funkcia
def test_classifier():
    """Testovanie klasifik√°tora"""
    import logging
    logging.basicConfig(level=logging.INFO)

    print("üß™ Testovanie NewsClassifier...")

    # Vytvorenie klasifik√°tora
    classifier = create_classifier()

    # Inform√°cie o modeli
    info = classifier.get_model_info()
    print(f"üìä Model info: {info}")

    # Testovacie pr√≠klady
    test_texts = [
        "≈†okuj√∫ce odhalenie v Bratislave!",
        "Vl√°da schv√°lila nov√Ω rozpoƒçet",
        "Tajn√© spolky ovl√°daj√∫ parlament",
        "Nov√Ω z√°kon zakazuje bicykle"
    ]

    print("\nüîç Testovacie predikcie:")
    for text in test_texts:
        try:
            category, probs = classifier.predict(text)
            print(f"  '{text}' -> {category} (istota: {max(probs.values()):.3f})")
        except Exception as e:
            print(f"  ‚ùå Chyba pre '{text}': {e}")


if __name__ == "__main__":
    test_classifier()